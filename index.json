
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I’m Ashley Hill, a Brit who has spent his life in France. Currently a Researcher Engineer specialized in machine learning applied to robotics. You can usually find me playing around with Machine Learning, Robotics, Electronics, Astronomy, and Programming.\nHere is my thesis for the curious.\n","date":1669896000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1669896000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m Ashley Hill, a Brit who has spent his life in France. Currently a Researcher Engineer specialized in machine learning applied to robotics. You can usually find me playing around with Machine Learning, Robotics, Electronics, Astronomy, and Programming.","tags":null,"title":"Ashley W.D. Hill","type":"authors"},{"authors":["Ashley W.D. Hill","Jean Laneurit","Roland Lenain","Eric Lucet"],"categories":null,"content":"","date":1669896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669896000,"objectID":"706ddabee8b5cb67b1e95b017b33a2ce","permalink":"https://hill-a.me/publication/agri2022/","publishdate":"2022-12-01T12:00:00Z","relpermalink":"/publication/agri2022/","section":"publication","summary":"This paper addresses the problem of adapting a control system to unseen conditions, specifically to the problem of trajectory tracking in off-road conditions. Three different approaches are considered and compared for this comparative study: The first approach is a classical reinforcement learning method to define the steering control of the system. The second strategy uses an end-to-end reinforcement learning method, allowing for the training of a policy for the steering of the robot. The third strategy uses a hybrid gain tuning method, allowing for the adaptation of the settling distance with respect to the robot’s capabilities according to the perception, in order to optimize the robot’s behavior with respect to an objective function. The three methods are described and compared to the results obtained using constant parameters in order to identify their respective strengths and weaknesses. They have been implemented and tested in real conditions on an off-road mobile robot with variable terrain and trajectories. The hybrid method allowing for an overall reduction of 53.2% when compared with a predictive control law. A thorough analysis of the methods are then performed, and further insights are obtained in the context of gain tuning for steering controllers in dynamic environments. The performance and transferability of these methods are demonstrated, as well as their robustness to changes in the terrain properties. As a result, tracking errors are reduced while preserving the stability and the explainability of the control architecture.","tags":["Reinforcement Learning","Evolutionary Strategy","Mobile Robotics","Robotics","Uncertainty","Neural networks","Vehicle dynamics"],"title":"Online Gain Tuning Using Neural Networks: A Comparative Study","type":"publication"},{"authors":["Ashley W.D. Hill","Jean Laneurit","Roland Lenain","Eric Lucet"],"categories":null,"content":"","date":1656676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656676800,"objectID":"a65705991a78ad4124070aad856db0c1","permalink":"https://hill-a.me/publication/ram2022/","publishdate":"2022-07-01T12:00:00Z","relpermalink":"/publication/ram2022/","section":"publication","summary":"This paper addresses the problem of the on-line adaptation of control parameters, dedicated to a path tracking problem in off-road conditions. Two approaches are proposed to modify the tuning gain of a previously developed adaptive and predictive control law. The first approach is a deterministic method based on the dynamic equations of the system, allowing the adaptation of the settling distance with respect to the robot capabilities depending on the grip conditions and velocity. The second strategy uses a neural network trained with a Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to optimize the robot's behavior with respect to an objective function. Each approach uses as input dynamic parameters, estimated from sliding angles and cornering stiffness observers. Both methods are described and compared to results obtained when using constant parameters in order to identify their respective strengths and weaknesses. They have been implemented and tested in real conditions on an off-road mobile robot with varying terrain and trajectories. An in depth analysis of the proposed methods is done, and further insights are obtained in the context of gain tuning for steering controllers in dynamic environments. The performance and transferability of these methods are demonstrated, as well as their robustness to changes in the terrain properties. As a result, tracking errors are reduced while preserving the stability and the explainability of the control architecture.","tags":["Reinforcement Learning","Evolutionary Strategy","Mobile Robotics","Robotics","Uncertainty","Neural networks","Vehicle dynamics"],"title":"Online Tuning of Control Parameters for Off-Road Mobile Robots: Novel Deterministic and Neural Network-Based Approaches","type":"publication"},{"authors":null,"categories":null,"content":"YACHT YACHT(https://github.com/ShadowMitia/YACHT) is a C/C++ project template based on CMake designed by ShadowMitia and I. Its goal is to avoid boilerplate CMake code, when trying to add extra features to your project. It uses a “pay for what you use” style of implementation, meaning it has a very small file size and does not generate file for features you didn’t ask for.\nIt has support for:\nEasy compiler flags \u0026amp; feature configuration Linters and formatting tools: clang-tidy, cppcheck, include-what-you-use, \u0026amp; clang-format Integrated testing libraries: GTest \u0026amp; Catch2 Package managers: conan \u0026amp; vcpkg Crossplatform support: Unix Make, Ninja, \u0026amp; Windows Visual Studio 2015 and up Dev perks: Docker, a fully featured ./build.sh script, a predefined .gitignore, \u0026amp; a pre setup folder structure. Being based on CMake, it also allows you to continue to use your custom *.cmake scripts and to alter any parts of the CMakeLists.txt with little interference.\n","date":1654128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654128000,"objectID":"42178e5cadc2225407de7830228e0f54","permalink":"https://hill-a.me/project/yacht/","publishdate":"2022-06-02T00:00:00Z","relpermalink":"/project/yacht/","section":"project","summary":"Yet Another C++ Helper Template, a template for starting C/C++ projects","tags":["C-Cpp","Docker","unix","windows"],"title":"YACHT","type":"project"},{"authors":["Ashley W.D. Hill","Eric Lucet","Roland Lenain"],"categories":null,"content":"","date":1641038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641038400,"objectID":"aa36417a59f7821598c00855503440cd","permalink":"https://hill-a.me/publication/lnee2022/","publishdate":"2022-01-01T12:00:00Z","relpermalink":"/publication/lnee2022/","section":"publication","summary":"In the paper, a novel gradient-based feature importance method for neural networks is described. This method is compared to the existing feature importance method using a trained neural network, which predicts the optimal gains in real time, for a steering controller on a mobile robot. The neural network is trained using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to minimize an objective function. From an analysis using the feature importance methods, key inputs are determined, and their contribution to the neural network’s prediction are observed. Furthermore, using a first-order Taylor approximation of the neural network, an improved control law is determined and tested based on the results of the gradient-based feature importance method. This analysis is then applied to an existing neural network using real-world experiments, in order to determine the behavior of the gains with respect to each input, and allows for a glimpse into the neural network’s inner workings in order to improve its explainability.","tags":["Machine Learning","Neural Network","Mobile Robotics","Robotics","Control Theory","Gain Tuning","Adaptive Control","Explainable Artificial Intelligence"],"title":"A Novel Gradient Feature Importance Method for Neural Networks: An Application to Controller Gain Tuning for Mobile Robots","type":"publication"},{"authors":["François Gauthier-Clerc","Ashley W.D. Hill","Jean Laneurit","Roland Lenain","Eric Lucet"],"categories":null,"content":"","date":1622376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622376000,"objectID":"05a04cf65337b090e6c464cd8fda3e1e","permalink":"https://hill-a.me/publication/icra2021/","publishdate":"2021-05-30T12:00:00Z","relpermalink":"/publication/icra2021/","section":"publication","summary":"During the off-road path following of a wheeled mobile robot in presence of poor grip conditions, the longitudinal velocity should be limited in order to maintain safe navigation with limited tracking errors, while at the same time being high enough to minimize travel time. Thus, this paper presents a new approach of online speed fluctuation, capable of limiting the lateral error below a given threshold, while maximizing the longitudinal velocity. This is accomplished using a neural network trained with a reinforcement learning method. This speed modulation is done side-by-side with an existing model-based predictive steering control, using a state estimator and dynamic observers. Simulated and experimental results show a decrease in tracking error, while maintaining a consistent travel time when compared to a classical constant speed method and to a kinematic speed fluctuation method.","tags":["Reinforcement Learning","Evolutionary Strategy","Mobile Robotics","Robotics","online velocity fluctuation","Neural networks","Vehicle dynamics"],"title":"Online velocity fluctuation of off-road wheeled mobile robots: A reinforcement learning approach","type":"publication"},{"authors":["Antonin Raffin","Ashley W.D. Hill","Adam Gleave","Anssi Kanervisto","Maximilian Ernestus","Noah Dormann"],"categories":null,"content":"","date":1609502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609502400,"objectID":"cb373283fd85c30b98e4c3c6972a042b","permalink":"https://hill-a.me/publication/stable-baselines/","publishdate":"2021-01-01T12:00:00Z","relpermalink":"/publication/stable-baselines/","section":"publication","summary":"Stable-Baselines3 provides open-source implementations of deep reinforcement learning (RL) algorithms in Python. The implementations have been benchmarked against reference codebases, and automated unit tests cover 95% of the code. The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare different RL algorithms. Our documentation, examples, and source-code are available at https://github.com/DLR-RM/stable-baselines3.","tags":["Reinforcement Learning","Baselines","Software","Open-Source","Python","PyTorch"],"title":"Stable-baselines3: Reliable reinforcement learning implementations","type":"publication"},{"authors":["Ashley W.D. Hill","Jean Laneurit","Roland Lenain","Eric Lucet"],"categories":null,"content":"","date":1603540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603540800,"objectID":"79ae958364dd0c00bfea24ea34adf8c8","permalink":"https://hill-a.me/publication/iros2020/","publishdate":"2021-02-10T12:00:00Z","relpermalink":"/publication/iros2020/","section":"publication","summary":"This paper proposes a new approach for online control law gains adaptation, through the use of neural networks and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to optimize the behavior of the robot with respect to an objective function. The neural network considered takes as input the current observed state as well as its uncertainty, and provides as output the control law gains. It is trained, using the CMA-ES algorithm, on a simulator reproducing the vehicle dynamics. Then, it is tested in real conditions on an agricultural mobile robot at different speeds. The transferability of this method from simulation to a real system is demonstrated, as well as its robustness to environmental changes, such as GPS signal degradation or ground variation. As a result, path following errors are reduced, while ensuring tracking stability.","tags":["Reinforcement Learning","Evolutionary Strategy","Mobile Robotics","Robotics","Uncertainty","Neural networks","Vehicle dynamics"],"title":"Online gain setting method for path tracking using CMA-ES: Application to off-road mobile robot control","type":"publication"},{"authors":["Ashley W.D. Hill","Eric Lucet","Roland Lenain"],"categories":null,"content":"","date":1594123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594123200,"objectID":"885b871e5458b73c69ca3097d2fc10a0","permalink":"https://hill-a.me/publication/icinco-2020/","publishdate":"2020-07-07T12:00:00Z","relpermalink":"/publication/icinco-2020/","section":"publication","summary":"This paper proposes a new approach for feature importance of neural networks and subsequently a methodology to determine useful sensor information in high performance controllers, using a trained neural network that predicts the quasi-optimal gain in real time. The neural network is trained using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to lower a given objective function. The important sensor information for robotic control are determined using the described methodology. Then a proposed improvement to the tested control law is given, and compared with the neural network's gain prediction method for real time gain tuning. As a results, crucial information about the importance of a given sensory information for robotic control is determined, and shown to improve the performance of existing controllers.","tags":["Machine Learning","Neural Network","Mobile Robotics","Robotics","Control Theory","Gain Tuning","Adaptive Control","Explainable Artificial Intelligence"],"title":"A new neural network feature importance method: Application to mobile robots controllers gain tuning","type":"publication"},{"authors":null,"categories":null,"content":"OBD logger This project is still a work in progress.\nThe goal is to monitor the car’s internal values, such as temperature, pressure, O2 meter, etc… To then be able to determine potential faults over time, and observer patterns in the data for extrapolation of engine characteristics.\nFurthermore, this unlocks the fault codes and the possibility of GPS logging, for analysis over time.\nAt the time of writing, a raspberry pi can connect to the OBD port, and log the desired values. The remote logging to a database is missing.\nHere is the code I wrote for connecting the the OBD connector in python. Use as you want, in MIT licence.\nimport serial from serial.tools import list_ports import json import numpy as np class obd_serial: \u0026#34;\u0026#34;\u0026#34; A serial interface for the OBD 2 protocol (https://en.wikipedia.org/wiki/OBD-II_PIDs) This class will connect to the device when created. :param device: (str) the OBD device (None for auto detect) :param baud: (int) the baud rate for the serial connection (default=38400) :param encoding: (str) the encoding type of the serial interface (default=\u0026#34;ascii\u0026#34;) \u0026#34;\u0026#34;\u0026#34; def __init__(self, device=None, baud=38400, encoding=\u0026#34;ascii\u0026#34;): # use autodetect if device is None: print(\u0026#34;auto-detecting device...\u0026#34;) devices = list_ports.comports() if devices == []: raise ValueError(\u0026#34;Cannot find any available com device at a baud rate of {}.\u0026#34;.format(baud)) found = False for dev in devices: valid, self.ser = self._check_serial(dev.device, baud) if valid: found = True print(\u0026#34;found ELM327 on {}\\r\\n\u0026#34;.format(dev.device)) break if not found: raise ValueError(\u0026#34;Cannot find valid ELM device at a baud rate of {}.\u0026#34;.format(baud)) else: valid, self.ser = self._check_serial(device, baud) if not valid: raise ValueError(\u0026#34;Device {} is not a valid ELM device at a baud rate of {}.\u0026#34;.format(device, baud)) self.encoding = encoding self._obd_lookup = json.load(open(\u0026#34;./obd_parsing.json\u0026#34;, \u0026#34;r\u0026#34;)) @classmethod def _check_serial(cls, dev, baud): \u0026#34;\u0026#34;\u0026#34; checks the serial connection for an ELM327 chip :param dev: (str) the device to check :param baud: (int) the baud rate to check :return: (bool, serial.Serial) return whether the serial port is valid, and if so the said connection \u0026#34;\u0026#34;\u0026#34; try: ser = serial.Serial(dev, baud, timeout=0.5) try: ser.write(b\u0026#34;atz\\r\\n\u0026#34;) ser.readline() # skip replay if ser.readline().startswith(b\u0026#34;ELM327\u0026#34;): ser.readline() ser.readline() ser.timeout = None return (True, ser) else: return (False, None) except serial.SerialException as e: print(e) ser.close() return (False, None) except serial.SerialException as e: print(e) return (False, None) def _decoded_message(self, message, anwser): \u0026#34;\u0026#34;\u0026#34; Transformes an OBD query, into clear elements :param message: (str) the obd request :param anwser: ([int]) the obd anwser in int format :return: ({str: any}) the decoded obd anwser \u0026#34;\u0026#34;\u0026#34; info = self._obd_lookup[message.upper()] output = {\u0026#34;type\u0026#34;: info[\u0026#34;type\u0026#34;], \u0026#34;desc\u0026#34;: info[\u0026#34;desc\u0026#34;]} if info[\u0026#34;type\u0026#34;] in [\u0026#34;support\u0026#34;, \u0026#34;action\u0026#34;]: # ignore pass elif info[\u0026#34;type\u0026#34;] == \u0026#34;raw\u0026#34;: # just print out output[\u0026#34;value\u0026#34;] = anwser elif info[\u0026#34;type\u0026#34;] == \u0026#34;choice\u0026#34;: # choose one from many found = False for i in range(0, len(info[\u0026#34;values\u0026#34;]), 2): if info[\u0026#34;values\u0026#34;][i] == str(anwser[0]): output[\u0026#34;value\u0026#34;] = info[\u0026#34;values\u0026#34;][i+1] found = True break if not found: output[\u0026#34;value\u0026#34;] = \u0026#34;INVALID VALUE \u0026#39;{}\u0026#39;\u0026#34;.format(anwser[0]) elif info[\u0026#34;type\u0026#34;] == \u0026#34;eq\u0026#34;: # calculate from equation output[\u0026#34;value\u0026#34;] = [] output[\u0026#34;unit\u0026#34;] = [] for i in range(0, len(info[\u0026#34;values\u0026#34;]), 2): output[\u0026#34;value\u0026#34;] += eval(info[\u0026#34;values\u0026#34;][i].format(*anwser)) output[\u0026#34;unit\u0026#34;] += info[\u0026#34;values\u0026#34;][i+1] elif info[\u0026#34;type\u0026#34;] == \u0026#34;bit\u0026#34;: # bit flag output[\u0026#34;value\u0026#34;] = [] for i in range(0, len(info[\u0026#34;values\u0026#34;]), 2): # get the byte number A:0, B:1, C:2, ... byte = anwser[ord(info[\u0026#34;values\u0026#34;][i][0].upper())-65] # get the associated bit, convert to boolean cond = bool((byte//2**int(info[\u0026#34;values\u0026#34;][i][1]))%2) output[\u0026#34;value\u0026#34;] += info[\u0026#34;values\u0026#34;][i+1] + \u0026#34;: {}\u0026#34;.format(cond) else: raise ValueError(\u0026#34;Type {} not implemented\u0026#34;.format(info[\u0026#34;type\u0026#34;])) return output def _parse_message(self, message, anwser): \u0026#34;\u0026#34;\u0026#34; Transformes an OBD query, into a human readable format. :param message: (str) the obd request :param anwser: ([int]) the obd anwser in int format :return: (str) the parsed, human readable anwser \u0026#34;\u0026#34;\u0026#34; info = self._decoded_message(message, anwser) output = info[\u0026#34;desc\u0026#34;] if info[\u0026#34;type\u0026#34;] in [\u0026#34;support\u0026#34;, \u0026#34;action\u0026#34;]: # ignore output += \u0026#34; (\u0026#34; + info[\u0026#34;type\u0026#34;] + \u0026#34;)\u0026#34; elif info[\u0026#34;type\u0026#34;] == \u0026#34;raw\u0026#34;: # just print out output += \u0026#34;\\r\\n\\t\u0026#34; + str(info[\u0026#34;value\u0026#34;]) elif info[\u0026#34;type\u0026#34;] == \u0026#34;choice\u0026#34;: # choose one from many output += \u0026#34;\\r\\n\\t\u0026#34; + info[\u0026#34;value\u0026#34;] elif info[\u0026#34;type\u0026#34;] == \u0026#34;eq\u0026#34;: # calculate from equation for i in range(len(info[\u0026#34;value\u0026#34;])): output += \u0026#34;\\r\\n\\t\u0026#34; + str(info[\u0026#34;value\u0026#34;][i]) + \u0026#34; \u0026#34; + info[\u0026#34;unit\u0026#34;][i] elif info[\u0026#34;type\u0026#34;] == \u0026#34;bit\u0026#34;: # bit flag for i in range(len(info[\u0026#34;value\u0026#34;])): output += \u0026#34;\\r\\n\\t\u0026#34; + info[\u0026#34;value\u0026#34;][i] else: raise ValueError(\u0026#34;Type {} not implemented\u0026#34;.format(info[\u0026#34;type\u0026#34;])) return output def close(self): \u0026#34;\u0026#34;\u0026#34; Closes the OBD connection \u0026#34;\u0026#34;\u0026#34; self.ser.close() def _query(self, message): \u0026#34;\u0026#34;\u0026#34; …","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591056000,"objectID":"f093eed5a2adf8f53c0f33ce3b327dce","permalink":"https://hill-a.me/project/obd/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/project/obd/","section":"project","summary":"An OBD logger for a car, allowing for recording key values over time","tags":["Embedded programming","unix","python","Soldering","Raspberry Pi","Electronic design"],"title":"OBD logger","type":"project"},{"authors":["Ashley W.D. Hill"],"categories":null,"content":"","date":1585990800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585990800,"objectID":"9fd3f928eb54521290cd137d4acebfd8","permalink":"https://hill-a.me/talk/ces-ia-reinforcement-learning-class/","publishdate":"2020-04-04T09:00:00Z","relpermalink":"/talk/ces-ia-reinforcement-learning-class/","section":"event","summary":"The CES-IA Reinforcement learning class slides.","tags":["Reinforcement Learning","Robotics"],"title":"CES-IA Reinforcement Learning Class","type":"event"},{"authors":["Antonin Raffin","Edward Beeching","Ashley W.D. Hill"],"categories":null,"content":"","date":1571389200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571389200,"objectID":"ab4ac85a6b30a4c91eb902a87a160836","permalink":"https://hill-a.me/talk/rl-tutorial-on-stable-baselines/","publishdate":"2019-10-18T09:00:00Z","relpermalink":"/talk/rl-tutorial-on-stable-baselines/","section":"event","summary":"Beginner tutorial on Stable Baselines library with colab notebooks","tags":["Reinforcement Learning"],"title":"RL Tutorial on Stable Baselines","type":"event"},{"authors":["Ashley W.D. Hill"],"categories":null,"content":"","date":1564401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564401600,"objectID":"c20b93c054022920b0073244770f13dd","permalink":"https://hill-a.me/talk/icinco-presentation/","publishdate":"2019-07-29T12:00:00Z","relpermalink":"/talk/icinco-presentation/","section":"event","summary":"The presentation of the 2019 ICINCO paper","tags":["Evolutionary Strategy","Reinforcement Learning","Mobile Robots","Robotics"],"title":"ICINCO presentation","type":"event"},{"authors":["Ashley W.D. Hill","Eric Lucet","Roland Lenain"],"categories":null,"content":"","date":1564401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564401600,"objectID":"1c3fe92f6eb07e42771548c6664d048d","permalink":"https://hill-a.me/publication/icinco-2019/","publishdate":"2019-07-29T12:00:00Z","relpermalink":"/publication/icinco-2019/","section":"publication","summary":"This paper proposes a method for dynamically varying the gains of a mobile robot controller that takes into account, not only errors to the reference trajectory but also the uncertainty in the localisation. To do this, the covariance matrix of a state observer is used to indicate the precision of the perception. CMA-ES, an evolutionary algorithm is used to train a neural network that is capable of adapting the robot's behaviour in real-time. Using a car-like vehicle model in simulation. Promising results show significant trajectory following performances improvements thanks to control gains fluctuations by using this new method. Simulations demonstrate the capability of the system to control the robot in complex environments, in which classical static controllers could not guarantee a stable behaviour.","tags":["Reinforcement Learning","Evolutionary Strategy","Mobile Robotics","Robotics"],"title":"Neuroevolution with CMA-ES for real-time gain tuning of a car-like robot controller","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://hill-a.me/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Antonin Raffin","Ashley W.D. Hill","René Traoré","Timothée Lesort","Natalia Díaz-Rodríguez","David Filliat"],"categories":null,"content":"","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548288000,"objectID":"6d42e2e062208177f5d86b35a3ff8f78","permalink":"https://hill-a.me/publication/feature-extraction/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/publication/feature-extraction/","section":"publication","summary":"Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. Against end-to-end learning, state representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning, and is robust to hyper-parameters change.","tags":["Reinforcement Learning,","State Representation Learning","Robotics"],"title":"Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics","type":"publication"},{"authors":["Antonin Raffin","Ashley W.D. Hill","René Traoré","Timothée Lesort","Natalia Díaz-Rodríguez","David Filliat"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"0edb8f334860f81d1bf3320939fd3327","permalink":"https://hill-a.me/publication/srl-toolbox/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/srl-toolbox/","section":"publication","summary":"State representation learning aims at learning compact representations from raw observations in robotics and control applications. Approaches used for this objective are auto-encoders, learning forward models, inverse dynamics or learning using generic priors on the state characteristics. However, the diversity in applications and methods makes the field lack standard evaluation datasets, metrics and tasks. This paper provides a set of environments, data generators, robotic control tasks, metrics and tools to facilitate iterative state representation learning and evaluation in reinforcement learning settings.","tags":["Reinforcement Learning,","State Representation Learning","Robotics"],"title":"S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State Representation Learning","type":"publication"},{"authors":null,"categories":null,"content":"Stable Baselines Github repository: https://github.com/hill-a/stable-baselines\nStable Baselines is a set of improved implementations of reinforcement learning algorithms based on OpenAI Baselines.\nYou can read a detailed presentation of Stable Baselines in the Medium article.\nThese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\nNote: despite its simplicity of use, Stable Baselines (SB) assumes you have some knowledge about Reinforcement Learning (RL). You should not utilize this library without some practice. To that extent, we provide good resources in the documentation to get started with RL.\nAcknowledgments Stable Baselines was created in the robotics lab U2IS (INRIA Flowers team) at ENSTA ParisTech.\nLogo credits: L.M. Tenkes\n","date":1530489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530489600,"objectID":"004002cb9541f4bf06ff2b18c7b8d978","permalink":"https://hill-a.me/project/stable-baselines/","publishdate":"2018-07-02T00:00:00Z","relpermalink":"/project/stable-baselines/","section":"project","summary":"Reinforcement learning library: a fork of OpenAI Baselines","tags":["Deep Learning","Machine Learning","Reinforcement Learning","Python"],"title":"Stable Baselines","type":"project"},{"authors":null,"categories":null,"content":"","date":1520208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520208000,"objectID":"c3fb3b79be6c5eacc05315b01f07641e","permalink":"https://hill-a.me/project/srl-toolbox/","publishdate":"2018-03-05T00:00:00Z","relpermalink":"/project/srl-toolbox/","section":"project","summary":"S-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) for Robotics","tags":["Deep Learning","machine learning","Reinforcement Learning","Python","State Representation Learning","Robotics"],"title":"S-RL Toolbox","type":"project"},{"authors":null,"categories":null,"content":"Portable Pi This project was born from a desire to create a portable raspberry pi, for retro gaming, and as a remote shell to connect to my home server.\nIt is based on the Adafruit DPI TFT for the GPIO to DPI connector, which was redesigned and built by hand. The power system is based on three reused laptop 18650 cells in parallel, with two boost converters in series: the first 2.8v-4.2v to 5v for powering the raspberry pi, the second 5v to 22v for the backlight of the TFT screen.\nIt was designed like this to minimize the power usage of the system as whole, and allowed for around 8h of high load, and a theoretical 16h of idle usage.\nThe extra parts visible are the USB lipo charger, and some mosfets to switch to USB power instead of the Lipo when charging.\nOverall, this project was not used daily, due to the large bulk. But I would like to redesign it one day with the Raspberry pi compute modules, which would allow for a slimer and lighter build.\n","date":1467417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467417600,"objectID":"0ff3f692ec0785f2ae31f1f2e18e0e24","permalink":"https://hill-a.me/project/portable_pi/","publishdate":"2016-07-02T00:00:00Z","relpermalink":"/project/portable_pi/","section":"project","summary":"A portable power efficient Raspberry pi","tags":["Soldering","Raspberry Pi","Electronic design","Unix"],"title":"Portable Pi","type":"project"},{"authors":null,"categories":null,"content":"S.E.A.T.A. S.E.A.T.A. is a robotics project where I participated in high school. The goal of the project was to design a robot from the ground up, that could follow a black line, avoid obstacles, and send a ball into a goal. This robot was then used in a county robotics contest between high schools, where S.E.A.T.A. won first place in speed.\nMy participation in the project consisted of designing the control board based on a ATMega8535, a power board, and a motor control board.\nThe control board A modular style of “plugs” that both supplied low current power and gave 8 pins of a port for communicating with the MCU was devised. It allowed for quick prototyping and debugging, while keeping the footprint of the board relatively small.\nAs the ATMega8535 only used 15mA of current, this setup was deemed sufficient for the robot.\nThe power \u0026amp; motor control board The most power hungry devise of the robot, was the two lateral motors. As such, the power regulation and motor control board were unified to limit potential power issues. However, this also caused some issues that needed to be solved due to the motors and the MCU sharing the same power rail.\nUnder the hood Demo Download the \u0026lt;a href=\u0026#34;2012-05-11 10.38.06.m4v\u0026#34;\u0026gt;MP4\u0026lt;/a\u0026gt; video. ","date":1338595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338595200,"objectID":"6262e96e6a8a6d33041e33f5d7a869b9","permalink":"https://hill-a.me/project/seata/","publishdate":"2012-06-02T00:00:00Z","relpermalink":"/project/seata/","section":"project","summary":"My first robotic project, High school.","tags":["microcontroller","Embedded programming","Robotics","C-Cpp","Soldering","Electronic design"],"title":"S.E.A.T.A ","type":"project"}]