[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m Ashley Hill, a brit who has spent his life in france. Currently a PhD student in Artificial Intelligence applied to Mobile Robots. You can usually find me playing around with Machine Learning, Robotics, Electronics, Astronomy, and Programming.\n","date":1585990800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585990800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://hill-a.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I\u0026rsquo;m Ashley Hill, a brit who has spent his life in france. Currently a PhD student in Artificial Intelligence applied to Mobile Robots. You can usually find me playing around with Machine Learning, Robotics, Electronics, Astronomy, and Programming.","tags":null,"title":"Ashley W.D. Hill","type":"authors"},{"authors":["Ashley W.D. Hill"],"categories":null,"content":"","date":1585990800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585990800,"objectID":"2dcc6468a657414d612f39c87217d5f5","permalink":"https://hill-a.me/talk/ces-ia/","publishdate":"2020-04-04T09:00:00Z","relpermalink":"/talk/ces-ia/","section":"talk","summary":"The CES-IA Reinforcement learning class slides.","tags":["reinforcement learning","robotics"],"title":"CES-IA Reinforcement Learning Class","type":"talk"},{"authors":["antonin raffin","edward beeching","Ashley W.D. Hill"],"categories":null,"content":"","date":1571389200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571389200,"objectID":"cbbb8c313d4bc11952b841f691fe9180","permalink":"https://hill-a.me/talk/rl-tuto-jnrr2019/","publishdate":"2019-10-18T09:00:00Z","relpermalink":"/talk/rl-tuto-jnrr2019/","section":"talk","summary":"Beginner tutorial on Stable Baselines library with colab notebooks","tags":["reinforcement learning"],"title":"RL Tutorial on Stable Baselines","type":"talk"},{"authors":["Ashley W.D. Hill"],"categories":null,"content":"","date":1564401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564401600,"objectID":"81ddd2a4655ecea164c493d8f6fb1d07","permalink":"https://hill-a.me/talk/icinco-2019/","publishdate":"2019-07-29T12:00:00Z","relpermalink":"/talk/icinco-2019/","section":"talk","summary":"The presentation of the 2019 ICINCO paper","tags":["evolutionary strategy","reinforcement learning","mobile robots","robotics"],"title":"ICINCO presentation","type":"talk"},{"authors":["Ashley W.D. Hill","eric lucet","roland lenain"],"categories":null,"content":"","date":1564401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564401600,"objectID":"1c3fe92f6eb07e42771548c6664d048d","permalink":"https://hill-a.me/publication/icinco-2019/","publishdate":"2019-07-29T12:00:00Z","relpermalink":"/publication/icinco-2019/","section":"publication","summary":"This paper proposes a method for dynamically varying the gains of a mobile robot controller that takes into account, not only errors to the reference trajectory but also the uncertainty in the localisation. To do this, the covariance matrix of a state observer is used to indicate the precision of the perception. CMA-ES, an evolutionary algorithm is used to train a neural network that is capable of adapting the robot's behaviour in real-time. Using a car-like vehicle model in simulation. Promising results show significant trajectory following performances improvements thanks to control gains fluctuations by using this new method. Simulations demonstrate the capability of the system to control the robot in complex environments, in which classical static controllers could not guarantee a stable behaviour.","tags":["reinforcement learning","evolutionary strategy","mobile robotics","robotics"],"title":"Neuroevolution with CMA-ES for real-time gain tuning of a car-like robot controller","type":"publication"},{"authors":["antonin raffin","Ashley W.D. Hill","rené traoré","timothée lesort","natalia díaz-rodríguez","david filliat"],"categories":null,"content":"","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548288000,"objectID":"6d42e2e062208177f5d86b35a3ff8f78","permalink":"https://hill-a.me/publication/feature-extraction/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/publication/feature-extraction/","section":"publication","summary":"Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. Against end-to-end learning, state representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning, and is robust to hyper-parameters change.","tags":["reinforcement learning,","state representation learning","robotics"],"title":"Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics","type":"publication"},{"authors":["antonin raffin","Ashley W.D. Hill","rené traoré","timothée lesort","natalia díaz-rodríguez","david filliat"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"0edb8f334860f81d1bf3320939fd3327","permalink":"https://hill-a.me/publication/srl-toolbox/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/srl-toolbox/","section":"publication","summary":"State representation learning aims at learning compact representations from raw observations in robotics and control applications. Approaches used for this objective are auto-encoders, learning forward models, inverse dynamics or learning using generic priors on the state characteristics. However, the diversity in applications and methods makes the field lack standard evaluation datasets, metrics and tasks. This paper provides a set of environments, data generators, robotic control tasks, metrics and tools to facilitate iterative state representation learning and evaluation in reinforcement learning settings.","tags":["reinforcement learning,","state representation learning","robotics"],"title":"S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State Representation Learning","type":"publication"},{"authors":null,"categories":null,"content":"Stable Baselines Github repository: https://github.com/hill-a/stable-baselines\nStable Baselines is a set of improved implementations of reinforcement learning algorithms based on OpenAI Baselines.\nYou can read a detailed presentation of Stable Baselines in the Medium article.\nThese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\nNote: despite its simplicity of use, Stable Baselines (SB) assumes you have some knowledge about Reinforcement Learning (RL). You should not utilize this library without some practice. To that extent, we provide good resources in the documentation to get started with RL.\nAcknowledgments Stable Baselines was created in the robotics lab U2IS (INRIA Flowers team) at ENSTA ParisTech.\nLogo credits: L.M. Tenkes\n","date":1530489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530489600,"objectID":"004002cb9541f4bf06ff2b18c7b8d978","permalink":"https://hill-a.me/project/stable-baselines/","publishdate":"2018-07-02T00:00:00Z","relpermalink":"/project/stable-baselines/","section":"project","summary":"Reinforcement learning library: a fork of OpenAI Baselines","tags":["deep learning","machine learning","reinforcement learning","python"],"title":"Stable Baselines","type":"project"},{"authors":null,"categories":null,"content":"","date":1520208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520208000,"objectID":"c3fb3b79be6c5eacc05315b01f07641e","permalink":"https://hill-a.me/project/srl-toolbox/","publishdate":"2018-03-05T00:00:00Z","relpermalink":"/project/srl-toolbox/","section":"project","summary":"S-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) for Robotics","tags":["deep learning","reinforcement learning","python","state representation learning","robotics"],"title":"S-RL Toolbox","type":"project"}]